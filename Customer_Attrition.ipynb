{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "###%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "import tensorflow as tf \n",
    "from sklearn.metrics import roc_curve, auc,roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip=pd.read_csv('D:/Prasanna/Prasanna/sepmodels/attr.csv',sep=\",\") \n",
    "oot=pd.read_csv('D:/Prasanna/Prasanna/sepmodels/oot_attr.csv',sep=\",\")  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "####cat list\n",
    "which_cat = ip.select_dtypes(include = [np.object]).columns.values \n",
    "## strip out dates, id's,acct numbers etc which are not deemed  categorical variables \n",
    "cat_list1 = which_cat.tolist() ## convert into a list\n",
    "cat_list1=[column for column in cat_list if column not in all_null_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_data_stats(data,cols):\n",
    "    column=data[cols]\n",
    "    cat_dict=dict(column.value_counts())\n",
    "    cat_missing_values=sum(pd.isnull(column))\n",
    "    cat_missing_perc=cat_missing_values/ip.shape[0]\n",
    "    return(cat_dict,cat_missing_values,cat_missing_perc)\n",
    " \n",
    "\n",
    "## Defines list to hold respective values\n",
    "   \n",
    "header=[]\n",
    "var_list=[]\n",
    "freq_list=[]\n",
    "percentage=[]\n",
    "mode_cat=[]\n",
    "cum_freq=[]\n",
    "cat_miss=[]\n",
    "cat_miss_percentage=[]\n",
    "cat_miss1=[]\n",
    "cat_miss_percentage1=[]\n",
    "\n",
    "\n",
    "## Iterate thru all column values to get the respective column statsistics\n",
    "for column_name in cat_list1:\n",
    "    #print(\"Column_name:\",column_name)\n",
    "    cat_dict,cat_missing_values,cat_missing_perc=cat_data_stats(ip,column_name)\n",
    "    cat_miss1.append(cat_missing_values)\n",
    "    cat_miss_percentage1.append(cat_missing_perc)\n",
    "    ### find the mode ..ie maximum number of occurances of the factors in \n",
    "    ### the categorical variable\n",
    "    mode_var=max(cat_dict, key=cat_dict.get)\n",
    "       \n",
    "    for Var_category in cat_dict.keys():\n",
    "        cat_miss.append(cat_missing_values)\n",
    "        cat_miss_percentage.append(cat_missing_perc)\n",
    "        header.append(column_name)               ## Adding  variable names \n",
    "        var_list.append(Var_category)            ## Appending values ie the key from dict\n",
    "\n",
    "    for frequency in cat_dict.values():  \n",
    "        freq_list.append(frequency)              ## Appending frequency\n",
    "        percentage.append(frequency/sum(cat_dict.values())) ##\n",
    "        mode_cat.append(mode_var)  ## Mode appended\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "##Creating  a PD to hold the cat column stats\n",
    "   \n",
    "#### 1) Create dictionary to hold respective stats      \n",
    "basic_stats_cat_rpt = {   'Variable'  : header,\n",
    "                      'Category'  : var_list,\n",
    "                      'Frequency' : freq_list,\n",
    "                      'Percentage': percentage,\n",
    "                      'Missing_Values':cat_miss,\n",
    "                      'Missing_values %':cat_miss_percentage,\n",
    "                      'Mode'      : mode_cat\n",
    "                      }\n",
    "\n",
    "###  2) Create a pandas dataframe using above dictionary\n",
    "\n",
    "basic_stats_cat = pd.DataFrame(basic_stats_cat_rpt, columns = \n",
    "['Variable','Category','Frequency','Percentage','Missing_Values','Missing_values %','Mode'])\n",
    "\n",
    "##Write as csv to a folder\n",
    "\n",
    "basic_stats_cat.to_csv('D:/Prasanna/Prasanna/sepmodels/basic_stats_cat.csv', encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_num = ip.select_dtypes(include = [np.float64, np.int64]).columns.values\n",
    "num_list1 = which_num.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hampel_lim (data,cols):\n",
    "    column=data[cols].tolist()\n",
    "    median=np.nanmedian(column)\n",
    "    MAD=np.nanmedian([abs(x - median) for x in column])\n",
    "    HUL=np.nanmedian(column)+3*(np.nanmedian([abs(x - median) for x in column]))\n",
    "    HLL=np.nanmedian(column)-3*(np.nanmedian([abs(x - median) for x in column]))\n",
    "    return(median,MAD,HUL,HLL)\n",
    "    \n",
    "    \n",
    "############  mu_3siglim function#########################################\n",
    "##input   :  dataframe, columns\n",
    "##output  :  Upper and lower limits    \n",
    "    \n",
    "def mu_3siglim(data,cols):\n",
    "    column=data[cols].tolist()\n",
    "    sd=np.nanstd(column,axis=0)\n",
    "    UL=np.nanmean(column)+3*sd # Upper limit\n",
    "    LL=np.nanmean(column)-3*sd # Lower limit\n",
    "    return(UL,LL)\n",
    " \n",
    "############  percentile function#########################################\n",
    "##input    :  dataframe, columns\n",
    "##output   : percentiles     \n",
    " \n",
    "def percentiles(data,cols):\n",
    "    column=data[cols]\n",
    "    return(np.nanpercentile(column, [1, 2, 5, 10, 20, 30, 40 , 50,\n",
    "                         60, 70, 75,80, 90, 95, 96, 97, 98,99]))\n",
    "\n",
    " \n",
    "########### Basic statistics function ###################################\n",
    "## input  : dataframe,columns\n",
    "## output : Number of records,minimum,maximum,ranges,sum,variance & standard deviation\n",
    " \n",
    "def basic_statistics(data,cols):\n",
    "    column=data[cols].tolist()\n",
    "    N=len(column)\n",
    "    minimum=np.nanmin(column)\n",
    "    maximum=np.nanmax(column)\n",
    "    ranges=maximum-minimum\n",
    "    sums=np.nansum(column)\n",
    "    mean=np.nanmean(column)\n",
    "    var=np.nanvar(column)\n",
    "    stddev=np.nanstd(column)\n",
    "    return(N,minimum,maximum,ranges,sums,mean,var,stddev)\n",
    "    \n",
    "########## Missing values function ########################################\n",
    "## input  : dataframe ,columns\n",
    "## output : Missing values  \n",
    "\n",
    "def missing_values(data,cols):\n",
    "    column=data[cols]\n",
    "    missing_value= len(column.index)-column.count()\n",
    "    return(missing_value)\n",
    "    \n",
    "########## Zero values function ########################################\n",
    "## input  : dataframe ,columns\n",
    "## output : Zero values  \n",
    "    \n",
    "    \n",
    "def zero_values(data,cols):\n",
    "    column=data[cols]\n",
    "    zero_val=(column == 0).astype(int).sum(axis=0)\n",
    "    return(zero_val)\n",
    "        \n",
    "\n",
    "\n",
    "###################################End###########################################\n",
    "\n",
    "#########Define lists to store basic stats.The lists have been named accordingly \n",
    "#########to store respective stats \n",
    "\n",
    "\n",
    "header=[]\n",
    "median_row=[]\n",
    "mad_row=[]\n",
    "HUL_row=[]\n",
    "HLL_row=[]\n",
    "UL_row=[]\n",
    "LL_row=[]    \n",
    "missing_row=[]\n",
    "zeros_row=[]\n",
    "N_row=[]\n",
    "min_row=[]\n",
    "max_row=[]\n",
    "percent_row=[]\n",
    "range_row=[]\n",
    "sum_row=[]\n",
    "mean_row=[]\n",
    "var_row=[]\n",
    "stddev_row=[]\n",
    "percent_1row=[]\n",
    "percent_2row=[]\n",
    "percent_5row=[]\n",
    "percent_10row=[]\n",
    "percent_20row=[]\n",
    "percent_30row=[]\n",
    "percent_40row=[]\n",
    "percent_50row=[]\n",
    "percent_60row=[]\n",
    "percent_70row=[]\n",
    "percent_75row=[]\n",
    "percent_80row=[]\n",
    "percent_90row=[]\n",
    "percent_95row=[]\n",
    "percent_96row=[]\n",
    "percent_97row=[]\n",
    "percent_98row=[]\n",
    "percent_99row=[]\n",
    "\n",
    "nochange=[]  \n",
    "binary_list=[]                   \n",
    "missing_percentage_row=[] \n",
    "sd_zero=[]\n",
    "\n",
    "se_row=[]\n",
    "CI_row=[]\n",
    "coef_var=[]\n",
    "\n",
    "kurtosis_row=[]\n",
    "skewness_row=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zcoe590\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\function_base.py:3250: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n",
      "C:\\Users\\zcoe590\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\lib\\nanfunctions.py:1545: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: RuntimeWarning: All-NaN axis encountered\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: RuntimeWarning: All-NaN axis encountered\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: Mean of empty slice\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:43: RuntimeWarning: Degrees of freedom <= 0 for slice.\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats \n",
    "from scipy import nanmean\n",
    "import scipy\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "df=ip\n",
    "for column_name in num_list1:\n",
    "    median,MAD,HUL,HLL=hampel_lim(df,column_name)\n",
    "    UL,LL= mu_3siglim(df,column_name)\n",
    "    percent=percentiles(df,column_name)\n",
    "    missing_value=missing_values(df,column_name)\n",
    "    zero_value=zero_values(df,column_name)\n",
    "    N,minimum,maximum,ranges,sums,mean,var,stddev=basic_statistics(df,column_name)\n",
    "    \n",
    "    ##  if maximum  = minimum then no change \n",
    "    ## then store value by appending    \n",
    "    if minimum==maximum:\n",
    "        nochange.append(column_name)\n",
    "    \n",
    "    ##  If sum  of count of zeros amd one are equal to count of number\n",
    "    ##  of rows then data is only binary  ( ie 0 and 1 only)\n",
    "    ##  then store value by appending        \n",
    "    if df[column_name].tolist().count(0) \\\n",
    "    +  df[column_name].tolist().count(1)==df.shape[0]:\n",
    "        binary_list.append(column_name)\n",
    "        \n",
    "    ## If standard deviation = 0 then store the value by appending         \n",
    "    if stddev==0:\n",
    "        sd_zero.append(column_name)\n",
    "    \n",
    "    ## Missing % = count of missing values/total number of rows\n",
    "    \n",
    "    missing_percentage=missing_value/(df.shape[0])  \n",
    "    header.append(column_name)\n",
    "    median_row.append(median)\n",
    "    mad_row.append(MAD)\n",
    "    HUL_row.append(HUL)\n",
    "    HLL_row.append(HLL)\n",
    "    \n",
    "    UL_row.append(UL)\n",
    "    LL_row.append(LL)\n",
    "    \n",
    "    missing_row.append(missing_value)\n",
    "    missing_percentage_row.append(missing_percentage)\n",
    "    zeros_row.append(zero_value)\n",
    "    N_row.append(N)\n",
    "    min_row.append(minimum)\n",
    "    max_row.append(maximum)\n",
    "    percent_row.append(percent)\n",
    "    range_row.append(ranges)\n",
    "    sum_row.append(sums)\n",
    "    mean_row.append(mean)\n",
    "    \n",
    "    ############SE,CI and coef_var #### needs to be imputed \n",
    "    se_row.append(\"None\")\n",
    "    CI_row.append(\"None\")\n",
    "    coef_var.append(\"None\")\n",
    "    ######################################################    \n",
    "    \n",
    "    var_row.append(var)\n",
    "    stddev_row.append(stddev)\n",
    "    percent_1row.append(percent[0])\n",
    "    percent_2row.append(percent[1])\n",
    "    percent_5row.append(percent[2])\n",
    "    percent_10row.append(percent[3])\n",
    "    percent_20row.append(percent[4])\n",
    "    percent_30row.append(percent[5])\n",
    "    percent_40row.append(percent[6])\n",
    "    percent_50row.append(percent[7])\n",
    "    percent_60row.append(percent[8])\n",
    "    percent_70row.append(percent[9])\n",
    "    percent_75row.append(percent[10])\n",
    "    percent_80row.append(percent[11])\n",
    "    percent_90row.append(percent[12])\n",
    "    percent_95row.append(percent[13])\n",
    "    percent_96row.append(percent[14])\n",
    "    percent_97row.append(percent[15])\n",
    "    percent_98row.append(percent[16])\n",
    "    percent_99row.append(percent[17])\n",
    "    df_no_missing=df[column_name].dropna()\n",
    "    kurtosis_row.append(scipy.stats.kurtosis(df_no_missing,fisher=False))\n",
    "    skewness_row.append(scipy.stats.skew(df_no_missing))\n",
    "    \n",
    "        \n",
    "#######Creating   PD dataframe containing basic stats############        \n",
    "### 1)  Each variable is stored as a dictionary ie example : key   : Mean\n",
    "###                                                          value : mean_row        \n",
    "        \n",
    "basic_stats_report = {'Variable': header,'Number of rows':N_row, 'Zeros'    : zeros_row,  \n",
    "                      'Missing' :missing_row,'Minimum'   :min_row,'Maximum' : max_row,\n",
    "                      'Range'   :range_row  ,'Sum'       :sum_row,'Median'  : median_row,\n",
    "                      'Mean'    :mean_row   ,'SE.mean'   :se_row ,'CI.mean.0.95':CI_row, \n",
    "                      'Variance':var_row    ,'stddev'    :stddev_row,'coef.var':coef_var,\n",
    "                      '1_Percentile'  : percent_1row,    '2_Percentile'  : percent_2row,\n",
    "                      '5_Percentile'  : percent_5row,    '10_Percentile' : percent_10row,\n",
    "                      '20_Percentile' : percent_20row,   '30_Percentile' : percent_30row,\n",
    "                      '40_Percentile' : percent_40row,   '50_Percentile' : percent_50row,\n",
    "                      '60_Percentile' : percent_60row,   '70_Percentile' : percent_70row,\n",
    "                      '75_Percentile' : percent_75row,   '80_Percentile' : percent_80row,\n",
    "                      '90_Percentile' : percent_90row,   '95_Percentile' : percent_95row,\n",
    "                      '96_Percentile' : percent_96row,   '97_Percentile' : percent_97row,\n",
    "                      '98_Percentile' : percent_98row,   '99_Percentile' : percent_99row,\n",
    "                      'UL'            : UL_row,'LL'      : LL_row,'MAD':mad_row,\n",
    "                      'HUL'           : HUL_row,'HLL'    : HLL_row\n",
    "                      \n",
    "                      }\n",
    "  \n",
    " \n",
    " \n",
    "basic_stats_num = pd.DataFrame(basic_stats_report,columns =['Variable','Number of rows','Zeros',  \n",
    "                                'Missing','Minimum','Maximum','Range','Sum','Median',\n",
    "                                'Mean','SE.mean','CI.mean.0.95','Variance'  ,'stddev','coef.var',\n",
    "                                '1_Percentile','2_Percentile','5_Percentile','10_Percentile',\n",
    "                                '20_Percentile','30_Percentile','40_Percentile','50_Percentile',\n",
    "                                '60_Percentile','70_Percentile','75_Percentile','80_Percentile',\n",
    "                                '90_Percentile','95_Percentile','96_Percentile','97_Percentile',\n",
    "                                '98_Percentile','99_Percentile','UL','LL','MAD','HUL','HLL'\n",
    "                                ]\n",
    "                                  )\n",
    "                      \n",
    "\n",
    "                     \n",
    "basic_stats_num.to_csv('D:/Prasanna/Prasanna/sepmodels/basic_stats_num.csv', encoding='utf-8')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of columns\n",
    "columns=ip.columns.tolist()\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/allvariables.csv', encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=q_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/qlist.csv', encoding='utf-8') \n",
    "\n",
    "columns=avg_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/avglist.csv', encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852\n",
      "238\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "## Get numerical variables \n",
    "which_num = ip[var_list].select_dtypes(include = [np.float64, np.int64]).columns.values\n",
    "num_list = which_num.tolist() ## convert into a list\n",
    "##remove all null\n",
    "all_null_list=[col for col in ip[var_list].columns if ip[col].isnull().all()]  \n",
    "num_list=[column for column in num_list if column not in all_null_list]\n",
    "\n",
    "####cat list\n",
    "which_cat = ip[var_list].select_dtypes(include = [np.object]).columns.values \n",
    "## strip out dates, id's,acct numbers etc which are not deemed  categorical variables \n",
    "cat_list = which_cat.tolist() ## convert into a list\n",
    "cat_list=[column for column in cat_list if column not in all_null_list]\n",
    "\n",
    "print(len(num_list))\n",
    "print(len(cat_list))\n",
    "print(len(all_null_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count list  387\n",
      "count  after dropping nulls greater than 80 % 281\n",
      "flag_cols  14\n",
      "count  after dropping nulls greater than 80 % 14\n",
      "Num_list is : 852\n",
      "Num_list is : 557\n",
      "count  after dropping nulls greater than 80 % 318\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "X_train, X_test, y_train, y_test = train_test_split(ip, ip['attrition_flag'], test_size=0.35, random_state=2018, stratify=ip[' attrition_flag'])\n",
    " \n",
    "#################count columns#################################################\n",
    "count_cols = [col for col in num_list if 'cnt' in col]\n",
    "print(\"count list \" , len(count_cols))\n",
    "\n",
    "count_list=[cols for cols in count_cols if X_train[cols].isnull().sum()/X_train.shape[0] <= 0.9]  \n",
    "print(\"count  after dropping nulls greater than 80 %\" , len(count_list))\n",
    "###################flag columns################################################\n",
    "flag_cols = [col for col in num_list if 'flag' in col]\n",
    "print(\"flag_cols \" , len(flag_cols))\n",
    "flag_list=[cols for cols in flag_cols if X_train[cols].isnull().sum()/X_train.shape[0] <= 0.9]  \n",
    "print(\"count  after dropping nulls greater than 80 %\" , len(flag_list))\n",
    "\n",
    "####################new num list###############################################\n",
    "\n",
    "print(\"Num_list is :\" , len(num_list))\n",
    "num_list=[cols for  cols in num_list if cols not in count_list]\n",
    "num_list=[cols for  cols in num_list if cols not in flag_list]\n",
    "print(\"Num_list is :\" , len(num_list))\n",
    "\n",
    "################### Num list null values ######################################\n",
    "\n",
    "num_list=[cols for cols in num_list if X_train[cols].isnull().sum()/X_train.shape[0] <= 0.9]  \n",
    "print(\"count  after dropping nulls greater than 80 %\" , len(num_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=num_list+count_list+\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/numlist_nulls.9.csv', encoding='utf-8') \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:4355: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cust_age    0\n",
      "tenure      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "################### replace null values #######################################\n",
    " \n",
    "for cols in median_list:\n",
    "    #print(X_train[cols].head(10))\n",
    "    X_train[cols].fillna(np.mean(X_train[cols]),inplace=True)\n",
    "    X_test[cols].fillna(np.mean(X_test[cols]),inplace=True)\n",
    "    oot[cols].fillna(np.mean(oot[cols]),inplace=True)\n",
    "\n",
    "nanCounter = np.isnan(X_train[median_list]).sum()    \n",
    "print(nanCounter)    \n",
    "\n",
    " \n",
    "\n",
    "for cols in num_list:\n",
    "    #print(X_train[cols].head(10))\n",
    "    X_train[cols].fillna(0,inplace=True)\n",
    "    X_test[cols].fillna(0,inplace=True)\n",
    "    oot[cols].fillna(0,inplace=True)\n",
    "for cols in count_list:\n",
    "    #print(X_train[cols].head(10))\n",
    "    X_train[cols].fillna(0,inplace=True)\n",
    "    X_test[cols].fillna(0,inplace=True)\n",
    "    oot[cols].fillna(0,inplace=True)\n",
    "for cols in flag_list:\n",
    "    #print(X_train[cols].head(10))\n",
    "    X_train[cols].fillna(0,inplace=True)\n",
    "    X_test[cols].fillna(0,inplace=True)\n",
    "    oot[cols].fillna(0,inplace=True)\n",
    "    \n",
    "################## check for null values ######################################    \n",
    "#nanCounter = np.isnan(X_train[num_list+count_list+flag_list]).sum()    \n",
    "#print(nanCounter)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of numerical features before checking for SD=0  318\n",
      "Total number of numerical features after checking for SD=0  318\n",
      "Total number of numerical features after checking for max=min  318\n",
      " Before unqiue value treatment : 318\n",
      " After unqiue value treatment : 318\n",
      " Total number of count features before checking for SD=0  281\n",
      "Total number of count features after checking for SD=0  281\n",
      "Total number of count  after checking for max=min  281\n",
      "Total number of flag features after checking for max=min  10\n"
     ]
    }
   ],
   "source": [
    " \n",
    "\n",
    "################### SD check   ################################################ \n",
    "print(\" Total number of numerical features before checking for SD=0 \" ,len(num_list))\n",
    "num_list=[column for column in num_list  if np.nanstd(X_train[column],axis=0) !=0.0 ]\n",
    "print(\"Total number of numerical features after checking for SD=0 \"  , len(num_list))\n",
    "\n",
    "num_list=[column for column in num_list  if max(X_train[column])!=min(X_train[column])]\n",
    "print(\"Total number of numerical features after checking for max=min \"  , len(num_list))\n",
    "\n",
    "print(\" Before unqiue value treatment :\",len(num_list))\n",
    "num_list=[column for column in num_list  if len(X_train[column].unique()) >= 2 ]\n",
    "print(\" After unqiue value treatment :\",len(num_list))\n",
    "\n",
    " \n",
    "print(\" Total number of count features before checking for SD=0 \" ,len(count_list))\n",
    "count_list=[column for column in count_list  if np.nanstd(X_train[column],axis=0) !=0.0 ]\n",
    "print(\"Total number of count features after checking for SD=0 \"  , len(count_list))\n",
    "\n",
    "count_list=[column for column in count_list  if max(X_train[column])!=min(X_train[column])]\n",
    "print(\"Total number of count  after checking for max=min \"  , len(count_list))\n",
    "\n",
    "\n",
    "\n",
    "[column for column in flag_list  if np.nanstd(X_train[column],axis=0) !=0.0 ]\n",
    "flag_list=[column for column in flag_list  if max(X_train[column])!=min(X_train[column])]\n",
    "print(\"Total number of flag features after checking for max=min \"  , len(flag_list))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=num_list+count_list+flag_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/numlistsdmaxmin.csv', encoding='utf-8') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=num_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/numlist_m1m2m3.csv', encoding='utf-8') \n",
    "\n",
    "\n",
    "columns=cat_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/catlistm1m2m3.csv', encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_list before 144\n",
      "count_list before 281\n",
      "flag_list before 14\n",
      "num_list After 120\n",
      "count_list After 247\n",
      "flag_list After 6\n"
     ]
    }
   ],
   "source": [
    "#if % of zeros is greater than 90% omit:\n",
    "print('num_list before',  len(num_list))\n",
    "print('count_list before', len(count_list))\n",
    "print('flag_list before' , len(flag_list))\n",
    "\n",
    "num_list  =[column for column in num_list   if ((X_train[column] == 0).astype(int).sum(axis=0)/X_train.shape[0]) <=.90]\n",
    "count_list=[column for column in count_list if ((X_train[column] == 0).astype(int).sum(axis=0)/X_train.shape[0]) <=.90]\n",
    "flag_list =[column for column in flag_list  if ((X_train[column] == 0).astype(int).sum(axis=0)/X_train.shape[0]) <=.90]\n",
    "\n",
    "print('num_list After',  len(num_list))\n",
    "print('count_list After', len(count_list))\n",
    "print('flag_list After' , len(flag_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=num_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/numlistzero.9.csv', encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 109 columns to remove.\n"
     ]
    }
   ],
   "source": [
    "# Threshold for removing correlated variables\n",
    "threshold = 0.9\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = X_train[num_list].corr().abs()\n",
    "corr_matrix.head()\n",
    "\n",
    "# Upper triangle of correlations\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper.head()\n",
    "\n",
    "# Select columns with correlations above threshold\n",
    "corr_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "#\n",
    "print('There are %d columns to remove.' % (len(corr_drop)))\n",
    "#\n",
    "#\n",
    " \n",
    "num_list=[cols for cols in num_list if cols not in corr_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrices\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from __future__ import division \n",
    "#import statsmodels.api as sm\n",
    "y_train_df=pd.DataFrame(y_train) \n",
    "#y_test_df.columns = ['dv'] \n",
    "VIF_train =  pd.concat([X_train[num_list].reset_index(drop=True),y_train_df], axis=1)\n",
    "vif_list=list(VIF_train.columns.values)\n",
    "print(len(vif_list))\n",
    "xclude_list=['attrition_flag']\n",
    "vif_list=[cols for cols in vif_list if cols not  in xclude_list]\n",
    "#print(vif_list)\n",
    "features=\"+\".join(vif_list)\n",
    "\n",
    "print(len(vif_list))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qaz=VIF_train.columns.tolist() \n",
    "y, X = dmatrices('attrition_flag ~' + features, VIF_train, return_type=\"dataframe\")\n",
    "vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "#print(vif)\n",
    "vif_pd=pd.DataFrame(list(zip(vif,X.columns.values.tolist())))\n",
    "vif_pd.columns=['vif_value','Feature']\n",
    "xclude_list=['Intercept']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = vif_pd[vif_pd['vif_value'] <= 50.00]['Feature'].tolist()\n",
    "var_list=[x for x in var_list if x not in xclude_list]\n",
    "\n",
    "print(\" Length of num variabls  after vif :\" , len(var_list)) \n",
    "num_list=var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list=X_train[num_list].columns[(X_train[num_list] < 0).any()].tolist()\n",
    "#log transsformation\n",
    "#for column in num_list:\n",
    "#    X_train[column +'_log']=np.log(1 + X_train[column]).astype(float)\n",
    "#    oot[column +'_log']    =np.log(1 + oot[column]).astype(float)\n",
    "#    X_test[column +'_log'] =np.log(1 + X_test[column]).astype(float)\n",
    "\n",
    "len(log_list)\n",
    "for column in log_list:\n",
    "    num_list.append(column+'_log')\n",
    "    X_train[column +'_log']=np.log(1 + X_train[column]).astype(float)\n",
    "    oot[column +'_log']    =np.log(1 + oot[column]).astype(float)\n",
    "    X_test[column +'_log'] =np.log(1 + X_test[column]).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled=X_train \n",
    "X_test_scaled=X_test\n",
    "oot_scaled=oot\n",
    "\n",
    "#scaler=MinMaxScaler()\n",
    "#X_train_scaled[num_list]=scaler.fit_transform(X_train[num_list])\n",
    "#X_test_scaled[num_list]=scaler.fit_transform(X_test[num_list]) \n",
    "#oot_scaled[num_list]=scaler.fit_transform(oot[num_list]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:5984: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2910: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "for col in num_list:\n",
    "    percentiles = X_train_scaled[col].quantile([0.01,0.99]).values\n",
    "    X_train_scaled[col][X_train_scaled[col] <= percentiles[0]] = percentiles[0]\n",
    "    X_train_scaled[col][X_train_scaled[col] >= percentiles[1]] = percentiles[1]\n",
    "\n",
    "    X_test_scaled[col][X_test_scaled[col] <= percentiles[0]] = percentiles[0]\n",
    "    X_test_scaled[col][X_test_scaled[col] >= percentiles[1]] = percentiles[1]    \n",
    "    \n",
    "    oot_scaled[col][oot_scaled[col] <= percentiles[0]] = percentiles[0]\n",
    "    oot_scaled[col][oot_scaled[col] >= percentiles[1]] = percentiles[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "##Function to define fisher score calculation\n",
    "def fishers_score(X_train,cols,dep_var):\n",
    "    df=X_train\n",
    "    x0=np.mean(df.loc[df[dep_var] == 0] [cols])\n",
    "    var0=np.var(df.loc[df[dep_var] == 0][cols])\n",
    "    x1=np.mean(df.loc[df[dep_var] == 1] [cols])\n",
    "    var1=np.var(df.loc[df[dep_var] == 1][cols])\n",
    "    try:\n",
    "        score=abs(x0-x1)/(var0+var1)**(1/2)\n",
    "    except ZeroDivisionError:\n",
    "        score = 0\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For all numerical variables calculate fishers score\n",
    "dv= 'attrition_flag'\n",
    "fishers=[]\n",
    "fishers_cutoff=.7\n",
    "for column in num_list:\n",
    "    #print(column)\n",
    "    fishers.append(fishers_score(X_train,column,dv))    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "fishers_dict =       {     'Variable'  : num_list,\n",
    "                           'fisher'    : fishers\n",
    "                      }\n",
    " \n",
    "fishers_pd=pd.DataFrame(fishers_dict,columns = (['Variable','fisher']))\n",
    "\n",
    "cutoff_rows=round((len(fishers)*fishers_cutoff)) + 1\n",
    "fishers_pd = fishers_pd .sort_values(by='fisher', ascending=0)[1:cutoff_rows]\n",
    "\n",
    "##########Check for correlations.Drop variable based on fisher scores\n",
    " \n",
    "    \n",
    "    \n",
    "num_list=fishers_pd['Variable'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "print(len(num_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=num_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/numlistfischer.csv', encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\zcoe590\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "Value=[]\n",
    "Bins=[]\t\n",
    "Non_events=[]\t\n",
    "Events=[]\t\n",
    "percent_of_Non_Events=[]\n",
    "percent_of_Events=[]\n",
    "WOE=[]\n",
    "IV=[]\n",
    "cat_woe_var=[]\n",
    "cat_woe_iv=[]\n",
    "\n",
    "\n",
    "\n",
    "#qaz=cat_list[0:5]\n",
    " \n",
    "for cat_var in cat_list:    \n",
    "    X_train[cat_var+'_woe']=X_train[cat_var]\n",
    "    #print(\"cat_var\",cat_var)\n",
    "    ## Create cross tab between the variable and dependent variable\n",
    "    cat_pd=pd.crosstab(X_train[cat_var],X_train['attrition_flag'], margins=True)\n",
    "    total_iv=0\n",
    "    total_events=0\n",
    "    total_non_events=0    \n",
    "    woe_var=0\n",
    "    iv_var=0\n",
    "    num=0\n",
    "    den=0    \n",
    "    cat_to_woe={} \n",
    "    #(For all the rows from the cross tab except for the last row which is \"ALL\")\n",
    "    for i in range(len(cat_pd.index)-1):        \n",
    "        Value.append(cat_pd.index[i])\n",
    "        Bins.append(i+1)\n",
    "        Non_events.append(cat_pd[0][i])\n",
    "        total_non_events+=cat_pd[0][i]\n",
    "        Events.append(cat_pd[1][i])\n",
    "        total_events+=cat_pd[1][i]\n",
    "        percent_of_Non_Events.append(cat_pd[0][i]/cat_pd[0][len(cat_pd.index)-1])\n",
    "        percent_of_Events.append(cat_pd[1][i]/cat_pd[1][len(cat_pd.index)-1])\n",
    "        num=cat_pd[0][i]/cat_pd[0][len(cat_pd.index)-1]\n",
    "        den=cat_pd[1][i]/cat_pd[1][len(cat_pd.index)-1]\n",
    "        if num==0:\n",
    "            woe_var=0\n",
    "        elif den==0:\n",
    "            woe_var= math.log(num/(den+.0001))\n",
    "        else:\n",
    "            woe_var=math.log(num/den)\n",
    "        WOE.append(woe_var)\n",
    "        iv_var=(cat_pd[0][i]/cat_pd[0][len(cat_pd.index)-1])-(cat_pd[1][i]/cat_pd[1][len(cat_pd.index)-1])\n",
    "        IV.append(iv_var*woe_var)\n",
    "        total_iv+=(iv_var*woe_var)\n",
    "        #print(\"sri krisna\")\n",
    "        cat_to_woe[cat_pd.index[i]]=woe_var  \n",
    "        X_train[cat_var+'_woe'] = X_train[cat_var].map(cat_to_woe)\n",
    "       \n",
    "        \n",
    "    #cat_header.append(\"\")\n",
    "    Value.append(cat_var)\n",
    "    Bins.append(cat_var)\n",
    "    Non_events.append(total_non_events)\n",
    "    Events.append(total_events)\n",
    "    percent_of_Non_Events.append(\"***\")\n",
    "    percent_of_Events.append(\"***\")\n",
    "    WOE.append(\"*** \")\n",
    "    IV.append(total_iv)\n",
    "    cat_woe_var.append(cat_var)\n",
    "    cat_woe_iv.append(total_iv)\n",
    "    for gap in range(0,3):\n",
    "        Value.append(\"   \")\n",
    "        Bins.append(\"   \")\n",
    "        Non_events.append(\"   \")\n",
    "        Events.append(\"    \")\n",
    "        percent_of_Non_Events.append(\"    \")\n",
    "        percent_of_Events.append(\"    \")\n",
    "        WOE.append(\"   \")\n",
    "        IV.append(\"    \")\n",
    "    Value.append(\"Value\")\n",
    "    Bins.append(\"Bins\")\n",
    "    Non_events.append(\"Non_events\")\n",
    "    Events.append(\"Events\")\n",
    "    percent_of_Non_Events.append(\"Percent_of_Non_Events\")\n",
    "    percent_of_Events.append(\"Percent_of_Events\")\n",
    "    WOE.append(\"Woe\")\n",
    "    IV.append(\"IV\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cat_woe_dict = {      'Value'      : Value,\n",
    "                      'Bins'      : Bins,\n",
    "                      'Non_events': Non_events,\n",
    "                      'Events'    : Events,\n",
    "                      'Percent_of_Non_Events'      : percent_of_Non_Events,\n",
    "                      'Percent_of_Events'         : percent_of_Events,\n",
    "                      'Woe'       : WOE,\n",
    "                      'IV'        : IV,  \n",
    "                       }\n",
    "\n",
    "###  2) Create a pandas dataframe using above dictionary\n",
    "\n",
    "cat_woe  = pd.DataFrame(cat_woe_dict, columns = \n",
    "['Value','Bins','Non_events','Events','Percent_of_Non_Events','Percent_of_Events',\n",
    "'Woe','IV'])        \n",
    "      \n",
    "           \n",
    "    \n",
    "cat_iv_dict={'Variable':cat_woe_var,'IV':cat_woe_iv}    \n",
    "cat_iv=pd.DataFrame(cat_iv_dict,columns=['Variable','IV'])\n",
    "cat_iv=cat_iv.sort_values(by='IV',ascending=0)\n",
    "#cat_iv=cat_iv.query('IV>.1') \n",
    "#cat_iv=cat_iv.query('IV!=inf') \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_iv=cat_iv.query('IV>.01') \n",
    "cat_iv=cat_iv.query('IV!=inf') \n",
    "cat_list=cat_iv['Variable'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=cat_list\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/catlist.01.csv', encoding='utf-8') \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=num_list+age_list+cat_list+flag_list\n",
    "features=num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#features=num_list+count_list+flag_list \n",
    "X_train_scaled=X_train_scaled[features]\n",
    "X_test_scaled=X_test_scaled[features]\n",
    "X_oot_scaled=oot_scaled[features]\n",
    "Y_oot=oot['attrition_flag']\n",
    " \n",
    "print(len(X_train_scaled.columns.tolist()))\n",
    "print(len(X_test_scaled.columns.tolist()))\n",
    "print(len(X_oot_scaled.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=features\n",
    "columns_dict={'columns': columns}\n",
    "columns_pd  = pd.DataFrame(columns_dict,columns = (['columns']))\n",
    "columns_pd.to_csv('D:/Prasanna/Prasanna/sepmodels/featureslist.csv', encoding='utf-8') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = pd.get_dummies(X_train_scaled)\n",
    "X_test_final  = pd.get_dummies(X_test_scaled)\n",
    "X_oot_final   = pd.get_dummies(X_oot_scaled)\n",
    "\n",
    "#X_train_final = X_train_scaled\n",
    "#X_test_final  = X_test_scaled\n",
    "#X_oot_final   = X_oot_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = X_train_final[var]\n",
    "test_features = X_test_final[var]\n",
    "oot_features=X_oot_final[var]\n",
    "\n",
    "\n",
    "features = X_train_final[var].columns.values \n",
    "for feature in features:\n",
    "    mean, std = train_features[feature].mean(), train_features[feature].std()\n",
    "    train_features.loc[:, feature] = (train_features[feature] - mean) / std\n",
    "    test_features.loc[:, feature] = (test_features[feature] - mean) / std \n",
    "    oot_features.loc[:, feature] = (oot_features[feature] - mean) / std \n",
    "\n",
    "train_features=train_features.as_matrix()                      \n",
    "test_features=test_features.as_matrix() \n",
    "oot_features=oot_features.as_matrix()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=pd.DataFrame()\n",
    "train_labels['attrition_flag']=y_train\n",
    "test_labels=pd.DataFrame()\n",
    "test_labels['attrition_flag']=y_test\n",
    "oot_labels=pd.DataFrame()\n",
    "oot_labels['attrition_flag']=oot['attrition_flag_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(batch_size, features, labels):\n",
    "    \"\"\"\n",
    "    Create batches of features and labels\n",
    "    :param batch_size: The batch size\n",
    "    :param features: List of features\n",
    "    :param labels: List of labels\n",
    "    :return: Batches of (Features, Labels)\n",
    "    \"\"\"\n",
    "    assert len(features) == len(labels)\n",
    "    # TODO: Implement batching\n",
    "    output_batches = []\n",
    "    \n",
    "    sample_size = len(features)\n",
    "    for start_i in range(0, sample_size, batch_size):\n",
    "        end_i = start_i + batch_size\n",
    "        batch = [features[start_i:end_i], labels[start_i:end_i]]\n",
    "        output_batches.append(batch)\n",
    "        \n",
    "    #return output_batches\n",
    "    return output_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=len(X_train_final[var].columns.tolist())\n",
    "n_labels=1\n",
    " \n",
    "beta=.01\n",
    "\n",
    "''' Three layer model\n",
    "r = (30/2)**(1/4) = 1.9679896712654303 = 2\n",
    "HL1 = 2*2**3 = 16\n",
    "HL2 = 2*2**2 = 8\n",
    "HL3 = 2*2**1 = 4\n",
    "'''\n",
    "\n",
    "n_hidden_nodes1 =27\n",
    "n_hidden_nodes2 = 9\n",
    "n_hidden_nodes3 = 3\n",
    "\n",
    "features = tf.placeholder(tf.float32,[None,n_features])\n",
    "labels = tf.placeholder(tf.float32,[None,n_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# layer 1\n",
    "keep_prob = tf.placeholder(tf.float32) \n",
    "\n",
    "hidden_layer1_weights=tf.Variable(tf.truncated_normal([n_features,n_hidden_nodes1],stddev = 0.15))\n",
    "hidden_layer1_bias=tf.Variable(tf.truncated_normal([n_hidden_nodes1]))\n",
    "#hidden_layer1_weights=tf.Variable(tf.truncated_normal([n_features,n_labels],stddev = 0.15))\n",
    "#hidden_layer1_bias=tf.Variable(tf.truncated_normal([n_labels]))\n",
    "hidden_layer1=tf.nn.sigmoid(tf.add(tf.matmul(features,hidden_layer1_weights),hidden_layer1_bias))\n",
    "hidden_layer1 = tf.nn.dropout(hidden_layer1, keep_prob)\n",
    "\n",
    "print(hidden_layer1_weights.shape)\n",
    "print(hidden_layer1_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# layer 2\n",
    "\n",
    "hidden_layer2_weights=tf.Variable(tf.truncated_normal([n_hidden_nodes1,n_hidden_nodes2],stddev = 0.15))\n",
    "hidden_layer2_bias=tf.Variable(tf.truncated_normal([n_hidden_nodes2]))\n",
    "#hidden_layer2_weights=tf.Variable(tf.truncated_normal([n_hidden_nodes1,n_labels],stddev = 0.15))\n",
    "#hidden_layer2_bias=tf.Variable(tf.truncated_normal([n_labels]))\n",
    "hidden_layer2=tf.nn.sigmoid(tf.matmul(hidden_layer1,hidden_layer2_weights)+hidden_layer2_bias)\n",
    "hidden_layer2 = tf.nn.dropout(hidden_layer2, keep_prob)\n",
    "\n",
    "print(hidden_layer2_weights.shape)\n",
    "print(hidden_layer2_bias.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# layer 3\n",
    "\n",
    "#hidden_layer3_weights=tf.Variable(tf.truncated_normal([n_hidden_nodes2,n_labels],stddev = 0.15))\n",
    "#hidden_layer3_bias=tf.Variable(tf.truncated_normal([n_labels]))\n",
    "hidden_layer3_weights=tf.Variable(tf.truncated_normal([n_hidden_nodes2,n_hidden_nodes3],stddev = 0.15))\n",
    "hidden_layer3_bias=tf.Variable(tf.truncated_normal([n_hidden_nodes3]))\n",
    "hidden_layer3=tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer2,hidden_layer3_weights),hidden_layer3_bias))\n",
    "hidden_layer3 = tf.nn.dropout(hidden_layer3, keep_prob)\n",
    "\n",
    "print(hidden_layer3_weights.shape)\n",
    "print(hidden_layer3_bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 4\n",
    "\n",
    "hidden_layer4_weights=tf.Variable(tf.truncated_normal([n_hidden_nodes3,n_labels],stddev = 0.15))\n",
    "hidden_layer4_bias=tf.Variable(tf.truncated_normal([n_labels]))\n",
    "hidden_layer4=tf.nn.sigmoid(tf.matmul(hidden_layer3,hidden_layer4_weights)+hidden_layer4_bias)\n",
    "hidden_layer4 = tf.nn.dropout(hidden_layer4, keep_prob)\n",
    "\n",
    "print(hidden_layer4_weights.shape)\n",
    "print(hidden_layer4_bias.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=.000001   \n",
    "#cost= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_layer4, labels=labels))\n",
    "cost= tf.nn.sigmoid_cross_entropy_with_logits(logits=hidden_layer4, labels=labels)\n",
    "#regularizers = (tf.nn.l2_loss(hidden_layer1_weights) + tf.nn.l2_loss(hidden_layer2_weights))/2 \n",
    "#cost = tf.reduce_mean(cost + beta * regularizers)\n",
    "cost = tf.reduce_mean(cost)\n",
    "\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "optimizer =  tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2810*12\n",
    "\n",
    "sess=tf.Session()\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "init1=tf.initialize_local_variables()\n",
    "sess.run(init)\n",
    "sess.run(init1) \n",
    "\n",
    " \n",
    "\n",
    "for epoch_i in range(35000):\n",
    "    for batch_features, batch_labels in batches(batch_size, train_features, train_labels):\n",
    "        sess.run([optimizer], feed_dict={features: batch_features, labels: batch_labels,keep_prob:.7})\n",
    "   \n",
    "    if epoch_i % 100 == 0:\n",
    "        print(\"epoch\",epoch_i)\n",
    "        current_cost = sess.run(cost,feed_dict={features: batch_features, labels: batch_labels,keep_prob:1})\n",
    "        print(\"Training Current Cost:\" ,current_cost)\n",
    "         \n",
    "        pred = sess.run(hidden_layer4,feed_dict={features: train_features, labels: train_labels,keep_prob:1})\n",
    "        pred=pd.DataFrame(list(pred))[0]\n",
    "        print(\"AUC Score (Train): %f\" % roc_auc_score(train_labels['attrition_flag'], pred))\n",
    "        \n",
    "        pred = sess.run(hidden_layer4,feed_dict={features: oot_features, labels: oot_labels,keep_prob:1})\n",
    "        pred=pd.DataFrame(list(pred))[0]\n",
    "        print(\"AUC Score (Test): %f\" % roc_auc_score(oot_labels['attrition_flag'], pred))\n",
    "        current_cost = sess.run(cost,feed_dict={features: oot_features, labels: oot_labels,keep_prob:1})\n",
    "        print(\"Test Cost:\" ,current_cost)\n",
    "        \n",
    "pred=pd.DataFrame(list(pred))[0]\n",
    "print(\"AUC Score (Test): %f\" % roc_auc_score(test_labels['attrition_flag'], pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

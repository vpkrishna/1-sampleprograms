{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\" Using LSTM and tensorflow for end of month  balance prediction . Used 5 timesteps and 13 features  \"\n",
    "\n",
    "%reset -f\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf  \n",
    "from sklearn.preprocessing import MinMaxScaler , StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : (1166, 78)\n",
      "Testing  : (501, 78)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prasanna\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in sign\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(df1, df1['m6'], test_size=0.3, random_state=2018)\n",
    "print(\"Training :\" ,X_train_lstm.shape) \n",
    "print(\"Testing  :\" ,X_test_lstm.shape)\n",
    " \n",
    "X_train  =  X_train_lstm.copy()\n",
    "X_test   =  X_test_lstm.copy()     \n",
    "X_train[log_list]  = X_train_lstm[log_list].apply(lambda x: np.sign(x)*(np.log(np.abs(x) + 1)))\n",
    "X_test[log_list]   = X_test_lstm[log_list].apply(lambda x: np.sign(x)*(np.log(np.abs(x) + 1)))\n",
    "y_train  = y_train_lstm.apply(lambda x: np.sign(x)*(np.log(np.abs(x) + 1)))\n",
    "y_test   = y_test_lstm.apply(lambda x: np.sign(x)*(np.log(np.abs(x) + 1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dv=y_train.values\n",
    "lstm_cols=[cols for cols  in X_train.columns if 'eom' in cols and 'm1' not in cols]\n",
    "lstm_op=X_train[lstm_cols]\n",
    "train_cols=[cols for cols in X_train.columns if 'm6' not in cols]\n",
    " \n",
    "train_data=X_train[train_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_train=StandardScaler()\n",
    "train_data=pd.DataFrame(scaler_train.fit_transform(train_data), columns =train_data.columns.tolist())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.67437082, 8.67890725, 8.70775032, 8.67624219, 8.6047824 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler_op=StandardScaler()\n",
    "mean_std_op=scaler_op.fit(lstm_op)\n",
    "lstm_op=pd.DataFrame(scaler_op.fit_transform(lstm_op), columns =lstm_op.columns.tolist())\n",
    "mean_std_op.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(train_data,lstm_op, batch_size, seq_length ,num_inputs,num_outputs):\n",
    "    \n",
    "    \n",
    "    x1=train_data.values \n",
    "    y1=lstm_op.values\n",
    "    \n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    num_records = batch_size * seq_length\n",
    "    n_batches = len(x1)*seq_length//num_records\n",
    "    \n",
    "\n",
    "    #print(\"n_batches\" ,n_batches)\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    x1 = x1[:n_batches * batch_size]\n",
    "    y1 = y1[:n_batches * batch_size]\n",
    " \n",
    "    #print(x1.shape)    \n",
    "    # Reshape into batch_size rows\n",
    "    x1 = x1.reshape((batch_size, -1))\n",
    "    y1 = y1.reshape((batch_size, -1))\n",
    "    \n",
    "    for n in range(0, x1.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = x1[:, n:n+seq_length]\n",
    "        # The targets\n",
    "        stop =  int(seq_length/num_inputs)\n",
    "        start = int(n/num_inputs)\n",
    "        y = y1[:, start:start+stop]\n",
    "        \n",
    "        # For the very last batch, y will be one character short at the end of \n",
    "        # the sequences which breaks things. To get around this, I'll make an \n",
    "        # array of the appropriate size first, of all zeros, then add the targets.\n",
    "        # This will introduce a small artifact in the last batch, but it won't matter.\n",
    "         \n",
    "        #x=x.reshape(batch_size,n_steps,num_inputs)\n",
    "        #y=y.reshape(-1,num_inputs)\n",
    "        #print(int(seq_length /num_inputs))\n",
    "        x=x.reshape(batch_size,int(seq_length /num_inputs),num_inputs)\n",
    "        y=y.reshape(-1,num_outputs)\n",
    "        yield x, y\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(batch_size, num_steps,num_inputs,num_outputs):\n",
    "       # Declare placeholders we'll feed into the graph\n",
    "\n",
    "\n",
    "    inputs = tf.placeholder(tf.float32, [None, num_steps,num_inputs], name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, [None, num_outputs], name='targets')\n",
    "    \n",
    "    # Keep probability placeholder for drop out layers\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n",
    "       \n",
    "    def build_cell(lstm_size, keep_prob):\n",
    "        # Use a basic LSTM cell\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "        \n",
    "        # Add dropout to the cell\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([build_cell(lstm_size, keep_prob) for _ in range(num_layers)])\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    return cell, initial_state\n",
    "\"\"\" \n",
    " tf.nn.rnn_cell.MultiRNNCell, which takes a list of RNNCells as its inputs and wraps them into a single cell:\n",
    "\n",
    "cell = tf.nn.rnn_cell.MultiRNNCell([tf.nn.rnn_cell.BasicRNNCell(state_size)] * num_layers)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output, lstm_size, out_size):\n",
    "   \n",
    "    seq_output = tf.concat(lstm_output, axis=1)\n",
    "     \n",
    "    op = tf.reshape(seq_output, [-1, lstm_size])\n",
    "    \n",
    "    # Connect the RNN outputs to a softmax layer\n",
    "    with tf.variable_scope('matmul'):\n",
    "        matmul_w = tf.Variable(tf.truncated_normal((lstm_size, out_size), stddev=0.1))\n",
    "        matmul_b =  tf.Variable(tf.zeros(out_size)) \n",
    "        #tf.Variable(tf.truncated_normal(out_size,stddev=.1))\n",
    "    \n",
    "    # Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "    # of rows of logit outputs, one for each step and sequence\n",
    "    prediction = tf.matmul(op, matmul_w) + matmul_b\n",
    "     \n",
    "    return (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss_optimizer(targets,prediction,learning_rate):\n",
    "    #targets1=tf.reshape(targets, [batch_size,num_steps,-1])\n",
    "    #targets1=tf.reshape(targets1, [-1,1])\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(prediction - targets))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return(loss,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bal_lstm:\n",
    "    \n",
    "    def __init__(self, out_size=1, \n",
    "                       batch_size=64,  \n",
    "                       num_steps=50,\n",
    "                       lstm_size=128, \n",
    "                       num_layers=2, seq_length=5,\n",
    "                       learning_rate=0.001,num_inputs=1,num_outputs=1 \n",
    "                       ):\n",
    "           \n",
    "        tf.reset_default_graph()        \n",
    "        \n",
    "        # Build the input placeholder tensors\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps,num_inputs,num_outputs)\n",
    "        \n",
    "        # Build the LSTM cell\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "        self.lstm_outputs, state = tf.nn.dynamic_rnn(cell,self.inputs,initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        #Get   predictions  \n",
    "        self.prediction  = build_output(self.lstm_outputs, lstm_size, out_size)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss ,self.optimizer = build_loss_optimizer(self.targets,self.prediction,learning_rate)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of batches :: 11\n",
      "(100, 5, 13)\n",
      "[[[ 1.13829763 -0.18723929 -0.09613529  0.14970446 -0.00981461\n",
      "    4.25408392  0.60075279 -0.50982402  2.07447023  5.02754843\n",
      "   -0.07130663  0.13224163  1.12751989]\n",
      "  [ 1.87602168 -0.14186122 -0.02599943  0.13267013  0.12193032\n",
      "    1.55671379  0.88127277 -0.14644495  1.98268884  6.63335671\n",
      "   -0.17983412  0.13552211  1.23105918]\n",
      "  [ 1.2458321  -0.11874051 -0.02343834  0.11005274  0.03530104\n",
      "    3.38346554  0.94276045 -0.4261717   0.29892846  4.76564991\n",
      "    0.16754538  0.13652094  1.26657066]\n",
      "  [ 0.7863725  -0.09509291  0.47197749  0.11176771  0.0601339\n",
      "    1.58156719  1.09199648 -0.64155495  0.33750265  4.43724489\n",
      "   -0.16478629  0.13580928  1.27268899]\n",
      "  [ 3.77116263 -0.17886996 -0.08718766 -0.48669569 -2.59451067\n",
      "    2.66362051  0.64913983 -0.42295495  0.42712468  6.27067953\n",
      "   -0.18458534  0.15154847  1.07539688]]\n",
      "\n",
      " [[ 0.59985745 -0.18723929 -0.6510548   0.14970446 -0.23674757\n",
      "    0.00851656  0.19039023 -0.64154456 -0.1971068  -0.33701022\n",
      "   -0.08737311  0.13224163  0.8345075 ]\n",
      "  [ 0.06522592 -0.28905179 -0.60898675  0.13267013  0.03685697\n",
      "    0.08179915  0.22806941 -0.6550222  -0.15809142 -0.32462631\n",
      "   -0.11495602  0.13552211  0.89370022]\n",
      "  [ 0.64655808 -0.15647469 -0.60490861  0.11005274  0.02226227\n",
      "   -0.03083667  0.24115344 -0.68067423 -0.06389182 -0.35066817\n",
      "    0.70525194  0.13652094  1.09498762]\n",
      "  [-0.5244357  -0.09509291 -0.62702551  0.1735591   0.06677047\n",
      "   -0.1439603   0.54816856 -0.39661288 -0.07498733 -0.3363927\n",
      "    0.31212042  0.13580928  1.15800483]\n",
      "  [ 0.09100448 -0.17886996 -0.31310057  0.13531054  0.01522719\n",
      "   -0.40647119  0.16399693 -0.42295495 -0.07884538 -0.33938466\n",
      "   -0.1636588   0.15154847  1.11919472]]\n",
      "\n",
      " [[-0.4770229   0.2905273  -0.09613529  0.14970446  0.09890575\n",
      "    0.1524341  -0.19653776  0.14877869 -0.1971068  -0.33701022\n",
      "   -0.05984263  0.13224163  0.09722828]\n",
      "  [-0.53837267 -0.28905179 -0.60898675  0.13267013  0.13245101\n",
      "    0.32761826 -0.36835145  0.61642092 -0.15809142 -0.32462631\n",
      "   -0.17983412  0.13552211 -0.01766792]\n",
      "  [-0.55198997 -0.15647469 -0.60490861  0.11005274  0.05561618\n",
      "    0.51713776 -0.38325065 -0.4261717  -0.06389182 -0.35066817\n",
      "   -0.1574258   0.13652094 -0.01023191]\n",
      "  [-0.5244357  -0.09509291 -0.35227476  0.1735591   0.06677047\n",
      "    0.09681098 -0.35332004 -0.27414185 -0.07498733 -0.3363927\n",
      "   -0.1735277   0.13580928 -0.03622984]\n",
      "  [-0.52235521 -0.17886996 -0.08718766  0.13531054  0.17212455\n",
      "    0.47618017 -0.33914891  0.54869331 -0.07884538 -0.33938466\n",
      "   -0.18458534  0.15154847 -0.0176968 ]]]\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "batch_size=100 # number of records /batch\n",
    "num_steps=5   # number of sequence steps per batch\n",
    "num_inputs=13 # number of features\n",
    "seq_length=num_inputs * num_steps\n",
    "num_outputs=1\n",
    "for x, y in get_batches(train_data,lstm_op, batch_size, seq_length,num_inputs,num_outputs):\n",
    "    counter+=1\n",
    "    #print(\"x\",x)\n",
    "    #print(\"y\",y)\n",
    "    \n",
    "print(\"Total number of batches ::\", counter)    \n",
    "print(x.shape)\n",
    "print(x[0:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=900\n",
    "seq_length = num_inputs * num_steps # sequence length\n",
    "lstm_size  = 128      # size of hidden layers in LSTMs\n",
    "num_layers = 6        #Number of LSTM layers\n",
    "#learning_rate = 0.001  \n",
    "learning_rate=.0001    #Learning rate\n",
    "keep_prob     = 0.8      #Dropout keep probability\n",
    "out_size=1\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-37-37cc28e5a21c>:5: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-37-37cc28e5a21c>:13: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-40-527f45488f1e>:18: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\Prasanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\tensor_array_ops.py:162: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Prasanna\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Epoch: 10/900...  Training Step: 100...  Training loss: 0.621169031...  0.1299 sec/batch\n",
      "Epoch: 19/900...  Training Step: 200...  Training loss: 0.760043204...  0.1509 sec/batch\n",
      "Epoch: 28/900...  Training Step: 300...  Training loss: 0.599654436...  0.1269 sec/batch\n",
      "Epoch: 37/900...  Training Step: 400...  Training loss: 0.256552607...  0.1409 sec/batch\n",
      "Epoch: 46/900...  Training Step: 500...  Training loss: 0.249784127...  0.1279 sec/batch\n",
      "Epoch: 55/900...  Training Step: 600...  Training loss: 0.262842119...  0.1389 sec/batch\n",
      "Epoch: 64/900...  Training Step: 700...  Training loss: 0.341304898...  0.1209 sec/batch\n",
      "Epoch: 73/900...  Training Step: 800...  Training loss: 0.307666957...  0.1199 sec/batch\n",
      "Epoch: 82/900...  Training Step: 900...  Training loss: 0.238327846...  0.1279 sec/batch\n",
      "Epoch: 91/900...  Training Step: 1000...  Training loss: 0.186824143...  0.1599 sec/batch\n",
      "Epoch: 100/900...  Training Step: 1100...  Training loss: 0.230093494...  0.1519 sec/batch\n",
      "Epoch: 110/900...  Training Step: 1200...  Training loss: 0.101060003...  0.1429 sec/batch\n",
      "Epoch: 119/900...  Training Step: 1300...  Training loss: 0.249098063...  0.1539 sec/batch\n",
      "Epoch: 128/900...  Training Step: 1400...  Training loss: 0.277377367...  0.1499 sec/batch\n",
      "Epoch: 137/900...  Training Step: 1500...  Training loss: 0.166221097...  0.1469 sec/batch\n",
      "Epoch: 146/900...  Training Step: 1600...  Training loss: 0.203033537...  0.1719 sec/batch\n",
      "Epoch: 155/900...  Training Step: 1700...  Training loss: 0.240729362...  0.3798 sec/batch\n",
      "Epoch: 164/900...  Training Step: 1800...  Training loss: 0.312775254...  0.1469 sec/batch\n",
      "Epoch: 173/900...  Training Step: 1900...  Training loss: 0.275571227...  0.1479 sec/batch\n",
      "Epoch: 182/900...  Training Step: 2000...  Training loss: 0.214936480...  0.1469 sec/batch\n",
      "Epoch: 191/900...  Training Step: 2100...  Training loss: 0.170083880...  0.1909 sec/batch\n",
      "Epoch: 200/900...  Training Step: 2200...  Training loss: 0.204870909...  0.1339 sec/batch\n",
      "Epoch: 210/900...  Training Step: 2300...  Training loss: 0.101579368...  0.1419 sec/batch\n",
      "Epoch: 219/900...  Training Step: 2400...  Training loss: 0.212679118...  0.1659 sec/batch\n",
      "Epoch: 228/900...  Training Step: 2500...  Training loss: 0.245676503...  0.1519 sec/batch\n",
      "Epoch: 237/900...  Training Step: 2600...  Training loss: 0.157529771...  0.1469 sec/batch\n",
      "Epoch: 246/900...  Training Step: 2700...  Training loss: 0.201472595...  0.1569 sec/batch\n",
      "Epoch: 255/900...  Training Step: 2800...  Training loss: 0.199618235...  0.1909 sec/batch\n",
      "Epoch: 264/900...  Training Step: 2900...  Training loss: 0.272659868...  0.1399 sec/batch\n",
      "Epoch: 273/900...  Training Step: 3000...  Training loss: 0.254536450...  0.1349 sec/batch\n",
      "Epoch: 282/900...  Training Step: 3100...  Training loss: 0.196073398...  0.1399 sec/batch\n",
      "Epoch: 291/900...  Training Step: 3200...  Training loss: 0.162715539...  0.1229 sec/batch\n",
      "Epoch: 300/900...  Training Step: 3300...  Training loss: 0.192991346...  0.1529 sec/batch\n",
      "Epoch: 310/900...  Training Step: 3400...  Training loss: 0.091986731...  0.1539 sec/batch\n",
      "Epoch: 319/900...  Training Step: 3500...  Training loss: 0.198324233...  0.1399 sec/batch\n",
      "Epoch: 328/900...  Training Step: 3600...  Training loss: 0.235509440...  0.1349 sec/batch\n",
      "Epoch: 337/900...  Training Step: 3700...  Training loss: 0.160674259...  0.1409 sec/batch\n",
      "Epoch: 346/900...  Training Step: 3800...  Training loss: 0.198607773...  0.1529 sec/batch\n",
      "Epoch: 355/900...  Training Step: 3900...  Training loss: 0.196271256...  0.1199 sec/batch\n",
      "Epoch: 364/900...  Training Step: 4000...  Training loss: 0.261015832...  0.1219 sec/batch\n",
      "Epoch: 373/900...  Training Step: 4100...  Training loss: 0.249382406...  0.1229 sec/batch\n",
      "Epoch: 382/900...  Training Step: 4200...  Training loss: 0.204705551...  0.1379 sec/batch\n",
      "Epoch: 391/900...  Training Step: 4300...  Training loss: 0.156354055...  0.1459 sec/batch\n",
      "Epoch: 400/900...  Training Step: 4400...  Training loss: 0.190618232...  0.1249 sec/batch\n",
      "Epoch: 410/900...  Training Step: 4500...  Training loss: 0.088470727...  0.1209 sec/batch\n",
      "Epoch: 419/900...  Training Step: 4600...  Training loss: 0.186622649...  0.1339 sec/batch\n",
      "Epoch: 428/900...  Training Step: 4700...  Training loss: 0.232859671...  0.1389 sec/batch\n",
      "Epoch: 437/900...  Training Step: 4800...  Training loss: 0.154246166...  0.1479 sec/batch\n",
      "Epoch: 446/900...  Training Step: 4900...  Training loss: 0.195812002...  0.1549 sec/batch\n",
      "Epoch: 455/900...  Training Step: 5000...  Training loss: 0.203518480...  0.1599 sec/batch\n",
      "Epoch: 464/900...  Training Step: 5100...  Training loss: 0.267963052...  0.1609 sec/batch\n",
      "Epoch: 473/900...  Training Step: 5200...  Training loss: 0.243567169...  0.1559 sec/batch\n",
      "Epoch: 482/900...  Training Step: 5300...  Training loss: 0.197016180...  0.3248 sec/batch\n",
      "Epoch: 491/900...  Training Step: 5400...  Training loss: 0.153017581...  0.1469 sec/batch\n",
      "Epoch: 500/900...  Training Step: 5500...  Training loss: 0.191509977...  0.1309 sec/batch\n",
      "Epoch: 510/900...  Training Step: 5600...  Training loss: 0.085786447...  0.1449 sec/batch\n",
      "Epoch: 519/900...  Training Step: 5700...  Training loss: 0.186715946...  0.1639 sec/batch\n",
      "Epoch: 528/900...  Training Step: 5800...  Training loss: 0.225108966...  0.1599 sec/batch\n",
      "Epoch: 537/900...  Training Step: 5900...  Training loss: 0.156167537...  0.1309 sec/batch\n",
      "Epoch: 546/900...  Training Step: 6000...  Training loss: 0.198058680...  0.1339 sec/batch\n",
      "Epoch: 555/900...  Training Step: 6100...  Training loss: 0.196640521...  0.1439 sec/batch\n",
      "Epoch: 564/900...  Training Step: 6200...  Training loss: 0.259551108...  0.1459 sec/batch\n",
      "Epoch: 573/900...  Training Step: 6300...  Training loss: 0.237441331...  0.1769 sec/batch\n",
      "Epoch: 582/900...  Training Step: 6400...  Training loss: 0.186690718...  0.1899 sec/batch\n",
      "Epoch: 591/900...  Training Step: 6500...  Training loss: 0.155063003...  0.1659 sec/batch\n",
      "Epoch: 600/900...  Training Step: 6600...  Training loss: 0.181987360...  0.1369 sec/batch\n",
      "Epoch: 610/900...  Training Step: 6700...  Training loss: 0.093239322...  0.1449 sec/batch\n",
      "Epoch: 619/900...  Training Step: 6800...  Training loss: 0.172720611...  0.1799 sec/batch\n",
      "Epoch: 628/900...  Training Step: 6900...  Training loss: 0.224209338...  0.1709 sec/batch\n",
      "Epoch: 637/900...  Training Step: 7000...  Training loss: 0.142054155...  0.1579 sec/batch\n",
      "Epoch: 646/900...  Training Step: 7100...  Training loss: 0.190379754...  0.1739 sec/batch\n",
      "Epoch: 655/900...  Training Step: 7200...  Training loss: 0.181646556...  0.1419 sec/batch\n",
      "Epoch: 664/900...  Training Step: 7300...  Training loss: 0.249266043...  0.1469 sec/batch\n",
      "Epoch: 673/900...  Training Step: 7400...  Training loss: 0.242556989...  0.1299 sec/batch\n",
      "Epoch: 682/900...  Training Step: 7500...  Training loss: 0.187914267...  0.1309 sec/batch\n",
      "Epoch: 691/900...  Training Step: 7600...  Training loss: 0.142945871...  0.1339 sec/batch\n",
      "Epoch: 700/900...  Training Step: 7700...  Training loss: 0.173478469...  0.1439 sec/batch\n",
      "Epoch: 710/900...  Training Step: 7800...  Training loss: 0.084307209...  0.1419 sec/batch\n",
      "Epoch: 719/900...  Training Step: 7900...  Training loss: 0.180528909...  0.1389 sec/batch\n",
      "Epoch: 728/900...  Training Step: 8000...  Training loss: 0.198327512...  0.1329 sec/batch\n",
      "Epoch: 737/900...  Training Step: 8100...  Training loss: 0.136269987...  0.1319 sec/batch\n",
      "Epoch: 746/900...  Training Step: 8200...  Training loss: 0.186922073...  0.1289 sec/batch\n",
      "Epoch: 755/900...  Training Step: 8300...  Training loss: 0.185562983...  0.1249 sec/batch\n",
      "Epoch: 764/900...  Training Step: 8400...  Training loss: 0.253974617...  0.1219 sec/batch\n",
      "Epoch: 773/900...  Training Step: 8500...  Training loss: 0.238310993...  0.1329 sec/batch\n",
      "Epoch: 782/900...  Training Step: 8600...  Training loss: 0.173166454...  0.1499 sec/batch\n",
      "Epoch: 791/900...  Training Step: 8700...  Training loss: 0.144957289...  0.1629 sec/batch\n",
      "Epoch: 800/900...  Training Step: 8800...  Training loss: 0.183638126...  0.1379 sec/batch\n",
      "Epoch: 810/900...  Training Step: 8900...  Training loss: 0.089898884...  0.1909 sec/batch\n",
      "Epoch: 819/900...  Training Step: 9000...  Training loss: 0.174425051...  0.1229 sec/batch\n",
      "Epoch: 828/900...  Training Step: 9100...  Training loss: 0.187729746...  0.1329 sec/batch\n",
      "Epoch: 837/900...  Training Step: 9200...  Training loss: 0.142016873...  0.1559 sec/batch\n",
      "Epoch: 846/900...  Training Step: 9300...  Training loss: 0.192366689...  0.1439 sec/batch\n",
      "Epoch: 855/900...  Training Step: 9400...  Training loss: 0.178704754...  0.1379 sec/batch\n",
      "Epoch: 864/900...  Training Step: 9500...  Training loss: 0.239681289...  0.1449 sec/batch\n",
      "Epoch: 873/900...  Training Step: 9600...  Training loss: 0.232334226...  0.1359 sec/batch\n",
      "Epoch: 882/900...  Training Step: 9700...  Training loss: 0.168461144...  0.1219 sec/batch\n",
      "Epoch: 891/900...  Training Step: 9800...  Training loss: 0.139756575...  0.1259 sec/batch\n",
      "Epoch: 900/900...  Training Step: 9900...  Training loss: 0.171253651...  0.1279 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs =100\n",
    "np.random.seed(10)\n",
    "print_every_n = 100\n",
    "\n",
    "# Save every N iterations\n",
    "saver_every_n = 100\n",
    " \n",
    "model = bal_lstm(out_size=out_size, \n",
    "                batch_size=batch_size, \n",
    "                num_steps=num_steps,\n",
    "                lstm_size=lstm_size,\n",
    "                num_layers=num_layers,seq_length=seq_length, \n",
    "                learning_rate=learning_rate,num_inputs=num_inputs,num_outputs=num_outputs)\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        #print(\"sri krishna\")\n",
    "        for x, y in get_batches(train_data,lstm_op, batch_size, seq_length,num_inputs,num_outputs):\n",
    "                                 \n",
    "            # Train network\n",
    "            new_state = sess.run(model.initial_state)\n",
    "             \n",
    "            loss = 0\n",
    "            counter += 1\n",
    "            #print(\"counter\",counter)\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state} \n",
    "            \n",
    "             \n",
    "            batch_loss, new_state, predictions_train,_ = sess.run([model.loss, \n",
    "                                                 model.final_state,\n",
    "                                                 model.prediction, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            if (counter % print_every_n == 0):\n",
    "                end = time.time()\n",
    "                end = time.time()\n",
    "                print('Epoch: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Step: {}... '.format(counter),\n",
    "                      'Training loss: {:.9f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "                 \n",
    "            if (counter % saver_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))   \n",
    "    \n",
    "       \n",
    "     \n",
    "             \n",
    "     \n",
    "            #pred_train.append(predictions_test)\n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))  \n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints\\i500_l512.ckpt\n"
     ]
    }
   ],
   "source": [
    "## test test data\n",
    "batch_size =test_data.shape[0]\n",
    "pred_test=[]\n",
    "model =   bal_lstm(out_size=out_size, \n",
    "                  batch_size=batch_size, \n",
    "                  num_steps=num_steps,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers, \n",
    "                  learning_rate=learning_rate,num_inputs=num_inputs,num_outputs=num_outputs)\n",
    "saver = tf.train.Saver()\n",
    "test_acc = []\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints'))\n",
    "    for x, y in get_batches(test_data,lstm_op_test, batch_size, seq_length,num_inputs,num_outputs):\n",
    "         \n",
    "        new_state = sess.run(model.initial_state)\n",
    "        feed_train = {model.inputs: x,\n",
    "                        model.keep_prob: 1 ,\n",
    "                        model.initial_state: new_state}\n",
    "        predictions_test,new_state = sess.run([model.prediction,model.final_state],\n",
    "                                feed_dict=feed_train )\n",
    "        for n in range(0, predictions_test.ravel().shape[0],num_steps):\n",
    "            pred_test.append(predictions_test.ravel()[n+num_steps-1])\n",
    "        #print(predictions_train.ravel())    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test1= [(cols*mean_std_op_test.scale_[4] + mean_std_op_test.mean_[4] ) for cols in pred_test]\n",
    "pred_test2= [np.exp(cols) for cols in pred_test1]\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
